{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1856ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Optional: NLTK BLEU (more standard than re-implementing)\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "# import nltk; nltk.download(\"punkt\")  # <- run once if needed\n",
    "\n",
    "# BERTScore\n",
    "from bert_score import score as bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c77f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, math, sys, pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "def norm_key(s: str) -> str:\n",
    "    # collapse whitespace & lowercase; strip punctuation noise at ends\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    return s\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    t = t.strip().lower()\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "def flatten(d: Dict) -> Dict[str, str]:\n",
    "    \"\"\"Flatten your nested JSON (Patients/Doctors/Manufacturer/Regulators) into {question: answer}.\"\"\"\n",
    "    flat = {}\n",
    "    for section, qa in d.items():\n",
    "        if not isinstance(qa, dict): \n",
    "            continue\n",
    "        for q, a in qa.items():\n",
    "            flat[q] = a\n",
    "    return flat\n",
    "\n",
    "def bleu_per_item(preds: List[str], refs: List[str]) -> List[float]:\n",
    "    sm = SmoothingFunction().method1\n",
    "    scores = []\n",
    "    for p, r in zip(preds, refs):\n",
    "        scores.append(sentence_bleu([r.split()], p.split(), smoothing_function=sm))\n",
    "    return scores\n",
    "\n",
    "def bertscore_lists(preds: List[str], refs: List[str]):\n",
    "    P, R, F1 = bert_score(preds, refs, lang=\"en\", verbose=False)\n",
    "    return P.tolist(), R.tolist(), F1.tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfdfbfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " These prediction questions were not found in refs (after normalization):\n",
      " - What does the report say about the retainer ring being cracked or loose?\n"
     ]
    }
   ],
   "source": [
    "# ---------------- configure paths ----------------\n",
    "# Your model outputs (the JSON you pasted). Save it as 'preds.json'.\n",
    "PRED_PATH = Path(\"llm_query_answer_topten.json\")\n",
    "# Reference answers (gold). Save the JSON from section 2 as 'refs.json'.\n",
    "REF_PATH  = Path(\"answer_ref.json\")\n",
    "OUT_CSV   = Path(\"scores.csv\")\n",
    "\n",
    "# ---------------- load ----------------\n",
    "pred_raw = json.loads(PRED_PATH.read_text(encoding=\"utf-8\"))\n",
    "ref_raw  = json.loads(REF_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "preds_dict = flatten(pred_raw)\n",
    "refs_dict  = flatten(ref_raw)\n",
    "\n",
    "# Build normalization maps\n",
    "pred_map = {norm_key(k): k for k in preds_dict.keys()}\n",
    "ref_map  = {norm_key(k): k for k in refs_dict.keys()}\n",
    "\n",
    "# Intersect by normalized keys\n",
    "common_norm = [k for k in ref_map.keys() if k in pred_map]\n",
    "missing_in_preds = [ref_map[k] for k in ref_map.keys() if k not in pred_map]\n",
    "missing_in_refs  = [pred_map[k] for k in pred_map.keys() if k not in ref_map]\n",
    "\n",
    "if missing_in_preds:\n",
    "    print(\"These reference questions mistmatch with preds (after normalization):\")\n",
    "    for m in missing_in_preds: print(\" -\", m)\n",
    "if missing_in_refs:\n",
    "    print(\" These prediction questions were not found in refs (after normalization):\")\n",
    "    for m in missing_in_refs: print(\" -\", m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb21e8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus BLEU (mean sentence BLEU): 0.0035\n",
      "Mean BERTScore  P=0.8288  R=0.8549  F1=0.8415\n",
      "\n",
      "                                                                                            question     bleu   bert_P   bert_R  bert_F1\n",
      "                                   Are adhesive failures or peeling overlays reported in pump usage? 0.001843 0.824287 0.824589 0.824438\n",
      "         Are cosmetic damages like cracked battery tubes or retainer rings discussed in any reports? 0.011980 0.845396 0.884026 0.864280\n",
      "                          Are cracks in the device case mentioned in relation to patient complaints? 0.001780 0.837654 0.881060 0.858809\n",
      "      Are injury or malfunction related keywords present in reports marked where adverse event is No 0.001654 0.822655 0.866879 0.844188\n",
      "Are there any cases where the adverse event flag is marked as No but the predicted Adverse event ... 0.001659 0.820946 0.853353 0.836836\n",
      "                 Are there any reports describing nausea, vomiting, or hyperglycemia after pump use? 0.006256 0.835065 0.878615 0.856287\n",
      "                      Are there cases where alarm failures are linked to device malfunction or harm? 0.003852 0.829184 0.865248 0.846832\n",
      "                                Are there descriptions of ER visits due to insulin delivery failure? 0.004942 0.839686 0.880518 0.859618\n",
      "                                            Are there descriptions of broken or detached belt clips? 0.002874 0.828941 0.835659 0.832287\n",
      "                        Are there descriptions of delayed insulin delivery leading to hyperglycemia? 0.010009 0.823677 0.876338 0.849192\n",
      "                     Are there examples where predicted adverse event is Yes and ADVERSE flag is No? 0.000000 0.786787 0.858133 0.820913\n",
      "                                        Are users describing unresponsive or non-functional devices? 0.001911 0.828688 0.846333 0.837417\n",
      "                               Are users reporting device leaks or insulin delivery inconsistencies? 0.004040 0.839086 0.868257 0.853422\n",
      "            Can a device be tagged with physical_damage but still not be flagged as an Adverse event 0.001275 0.824431 0.856722 0.840266\n",
      "                            Do any patients describe removing or replacing the device due to issues? 0.005858 0.841475 0.836352 0.838906\n",
      "                                     Do any reports mention symptoms like fatigue, headache, or DKA? 0.003972 0.822767 0.864396 0.843068\n",
      "                   Does the FOI_TEXT describe harm or device failure even when ADVERSE event is No ? 0.002192 0.824685 0.867471 0.845537\n",
      "                                    Has the belt clip broken or cracked in real-world patient usage? 0.001699 0.844375 0.880076 0.861856\n",
      "                                 Was a patient hospitalized due to pump malfunction in any FOI text? 0.001977 0.827327 0.854992 0.840932\n",
      "                     What clinical symptoms are mentioned in the FOI_TEXT of malfunctioning devices? 0.001880 0.814649 0.846240 0.830144\n",
      "                          What does the FOI_TEXT say about the retainer ring being cracked or loose? 0.003908 0.830789 0.794369 0.812171\n",
      "                                           What does the report describe about motor or pump errors? 0.002318 0.841004 0.886383 0.863098\n",
      "                           What issues have patients reported with the insulin pump model MMT-1712K? 0.005665 0.820962 0.824676 0.822815\n",
      "                             What problems are described with insulin delivery in 640G INSULIN PUMP? 0.001659 0.819979 0.812209 0.816076\n",
      "                                 Which kinds of malfunctions are mentioned for PARADIGM PUMP models? 0.003439 0.846205 0.829096 0.837563\n",
      "\n",
      "Saved per-item scores to C:\\vaish\\scores.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "questions = [ref_map[k] for k in common_norm]  # use canonical (ref) wording\n",
    "y_ref = [normalize_text(refs_dict[ref_map[k]]) for k in common_norm]\n",
    "y_pred = [normalize_text(preds_dict[pred_map[k]]) for k in common_norm]\n",
    "\n",
    "# BLEU\n",
    "bleu_scores = bleu_per_item(y_pred, y_ref)\n",
    "bleu_avg = sum(bleu_scores)/len(bleu_scores) if bleu_scores else float(\"nan\")\n",
    "\n",
    "# BERTScore\n",
    "P, R, F1 = bertscore_lists(y_pred, y_ref)\n",
    "Pm, Rm, Fm = sum(P)/len(P), sum(R)/len(R), sum(F1)/len(F1)\n",
    "\n",
    "# Report\n",
    "df = pd.DataFrame({\n",
    "    \"question\": questions,\n",
    "    \"bleu\": bleu_scores,\n",
    "    \"bert_P\": P,\n",
    "    \"bert_R\": R,\n",
    "    \"bert_F1\": F1\n",
    "}).sort_values(\"question\")\n",
    "\n",
    "print(f\"\\nCorpus BLEU (mean sentence BLEU): {bleu_avg:.4f}\")\n",
    "print(f\"Mean BERTScore  P={Pm:.4f}  R={Rm:.4f}  F1={Fm:.4f}\\n\")\n",
    "print(df.to_string(index=False, max_colwidth=100))\n",
    "\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"\\nSaved per-item scores to {OUT_CSV.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd65cdb",
   "metadata": {},
   "source": [
    "### Scores on long worded reference answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb21bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- configure paths ----------------\n",
    "# Your model outputs (the JSON you pasted). Save it as 'preds.json'.\n",
    "PRED_PATH = Path(\"llm_query_answer_topten.json\")\n",
    "# Reference answers (gold). Save the JSON from section 2 as 'refs.json'.\n",
    "REF_PATH  = Path(\"long_answer_ref.json\")\n",
    "OUT_CSV   = Path(\"scores_long_ans.csv\")\n",
    "\n",
    "# ---------------- load ----------------\n",
    "pred_raw = json.loads(PRED_PATH.read_text(encoding=\"utf-8\"))\n",
    "ref_raw  = json.loads(REF_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "preds_dict = flatten(pred_raw)\n",
    "refs_dict  = flatten(ref_raw)\n",
    "\n",
    "# Build normalization maps\n",
    "pred_map = {norm_key(k): k for k in preds_dict.keys()}\n",
    "ref_map  = {norm_key(k): k for k in refs_dict.keys()}\n",
    "\n",
    "# Intersect by normalized keys\n",
    "common_norm = [k for k in ref_map.keys() if k in pred_map]\n",
    "missing_in_preds = [ref_map[k] for k in ref_map.keys() if k not in pred_map]\n",
    "missing_in_refs  = [pred_map[k] for k in pred_map.keys() if k not in ref_map]\n",
    "\n",
    "if missing_in_preds:\n",
    "    print(\"These reference questions mistmatch with preds (after normalization):\")\n",
    "    for m in missing_in_preds: print(\" -\", m)\n",
    "if missing_in_refs:\n",
    "    print(\" These prediction questions were not found in refs (after normalization):\")\n",
    "    for m in missing_in_refs: print(\" -\", m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d318c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus BLEU (mean sentence BLEU): 0.0104\n",
      "Mean BERTScore  P=0.8437  R=0.8755  F1=0.8591\n",
      "\n",
      "                                                                                            question     bleu   bert_P   bert_R  bert_F1\n",
      "                                   Are adhesive failures or peeling overlays reported in pump usage? 0.002998 0.827543 0.846378 0.836855\n",
      "         Are cosmetic damages like cracked battery tubes or retainer rings discussed in any reports? 0.018523 0.859474 0.902566 0.880493\n",
      "                          Are cracks in the device case mentioned in relation to patient complaints? 0.005208 0.845445 0.902300 0.872948\n",
      "      Are injury or malfunction related keywords present in reports marked where adverse event is No 0.012243 0.829888 0.868389 0.848702\n",
      "Are there any cases where the adverse event flag is marked as No\\n                  but the predi... 0.015436 0.843640 0.889254 0.865847\n",
      "                 Are there any reports describing nausea, vomiting, or hyperglycemia after pump use? 0.008233 0.848061 0.873359 0.860524\n",
      "                      Are there cases where alarm failures are linked to device malfunction or harm? 0.010242 0.836967 0.887993 0.861726\n",
      "                                Are there descriptions of ER visits due to insulin delivery failure? 0.007198 0.843297 0.897186 0.869407\n",
      "                                            Are there descriptions of broken or detached belt clips? 0.007643 0.846520 0.860021 0.853217\n",
      "                        Are there descriptions of delayed insulin delivery leading to hyperglycemia? 0.012740 0.851457 0.915739 0.882429\n",
      "                     Are there examples where predicted adverse event is Yes and ADVERSE flag is No? 0.008323 0.812049 0.862100 0.836326\n",
      "                                        Are users describing unresponsive or non-functional devices? 0.002361 0.827320 0.860177 0.843429\n",
      "                               Are users reporting device leaks or insulin delivery inconsistencies? 0.005591 0.850677 0.880182 0.865178\n",
      "            Can a device be tagged with physical_damage but still not be flagged as an Adverse event 0.005433 0.841183 0.888292 0.864096\n",
      "                            Do any patients describe removing or replacing the device due to issues? 0.014414 0.860955 0.871469 0.866180\n",
      "                                     Do any reports mention symptoms like fatigue, headache, or DKA? 0.017603 0.839472 0.890711 0.864333\n",
      "                   Does the FOI_TEXT describe harm or device failure even when ADVERSE event is No ? 0.012899 0.842030 0.886037 0.863473\n",
      "                                    Has the belt clip broken or cracked in real-world patient usage? 0.006088 0.849740 0.898917 0.873637\n",
      "                                 Was a patient hospitalized due to pump malfunction in any FOI text? 0.003824 0.824027 0.855441 0.839440\n",
      "                     What clinical symptoms are mentioned in the FOI_TEXT of malfunctioning devices? 0.005080 0.852212 0.890450 0.870911\n",
      "                          What does the FOI_TEXT say about the retainer ring being cracked or loose? 0.010877 0.856722 0.827993 0.842113\n",
      "                                           What does the report describe about motor or pump errors? 0.007973 0.840083 0.887664 0.863219\n",
      "                            What does the report say about the retainer ring being cracked or loose? 0.014276 0.859375 0.866206 0.862777\n",
      "                           What issues have patients reported with the insulin pump model MMT-1712K? 0.031070 0.858674 0.871394 0.864987\n",
      "                             What problems are described with insulin delivery in 640G INSULIN PUMP? 0.005372 0.831568 0.852661 0.841982\n",
      "                                 Which kinds of malfunctions are mentioned for PARADIGM PUMP models? 0.018291 0.858542 0.829071 0.843549\n",
      "\n",
      "Saved per-item scores to C:\\vaish\\scores_long_ans.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "questions = [ref_map[k] for k in common_norm]  # use canonical (ref) wording\n",
    "y_ref = [normalize_text(refs_dict[ref_map[k]]) for k in common_norm]\n",
    "y_pred = [normalize_text(preds_dict[pred_map[k]]) for k in common_norm]\n",
    "\n",
    "# BLEU\n",
    "bleu_scores = bleu_per_item(y_pred, y_ref)\n",
    "bleu_avg = sum(bleu_scores)/len(bleu_scores) if bleu_scores else float(\"nan\")\n",
    "\n",
    "# BERTScore\n",
    "P, R, F1 = bertscore_lists(y_pred, y_ref)\n",
    "Pm, Rm, Fm = sum(P)/len(P), sum(R)/len(R), sum(F1)/len(F1)\n",
    "\n",
    "# Report\n",
    "df = pd.DataFrame({\n",
    "    \"question\": questions,\n",
    "    \"bleu\": bleu_scores,\n",
    "    \"bert_P\": P,\n",
    "    \"bert_R\": R,\n",
    "    \"bert_F1\": F1\n",
    "}).sort_values(\"question\")\n",
    "\n",
    "print(f\"\\nCorpus BLEU (mean sentence BLEU): {bleu_avg:.4f}\")\n",
    "print(f\"Mean BERTScore  P={Pm:.4f}  R={Rm:.4f}  F1={Fm:.4f}\\n\")\n",
    "print(df.to_string(index=False, max_colwidth=100))\n",
    "\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"\\nSaved per-item scores to {OUT_CSV.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c155bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.54 seconds, 16.83 sentences/sec\n",
      "Saved full metrics to C:\\vaish\\score_analyse_long_ans.csv\n",
      "\n",
      "=== Lowest BERT F1 questions ===\n",
      "                                             question   bert_F1     chrF++  \\\n",
      "0   Are adhesive failures or peeling overlays repo...  0.061337  19.074895   \n",
      "10  Are there examples where predicted adverse eve...  0.071462  19.218829   \n",
      "24  What problems are described with insulin deliv...  0.072407  23.676587   \n",
      "25  Which kinds of malfunctions are mentioned for ...  0.082178  29.687894   \n",
      "20  What does the FOI_TEXT say about the retainer ...  0.095517  24.636944   \n",
      "\n",
      "    ROUGE-L_F1  \n",
      "0     0.083333  \n",
      "10    0.108108  \n",
      "24    0.141026  \n",
      "25    0.212766  \n",
      "20    0.128000  \n",
      "\n",
      "=== Lowest chrF++ questions ===\n",
      "                                             question   bert_F1     chrF++  \\\n",
      "18  Was a patient hospitalized due to pump malfunc...  0.102463  18.552426   \n",
      "0   Are adhesive failures or peeling overlays repo...  0.061337  19.074895   \n",
      "10  Are there examples where predicted adverse eve...  0.071462  19.218829   \n",
      "11  Are users describing unresponsive or non-funct...  0.108054  20.616260   \n",
      "6   Are there cases where alarm failures are linke...  0.209299  22.723709   \n",
      "\n",
      "    ROUGE-L_F1  \n",
      "18    0.087500  \n",
      "0     0.083333  \n",
      "10    0.108108  \n",
      "11    0.092715  \n",
      "6     0.092105  \n",
      "\n",
      "=== Lowest ROUGE-L questions ===\n",
      "                                             question   bert_F1     chrF++  \\\n",
      "0   Are adhesive failures or peeling overlays repo...  0.061337  19.074895   \n",
      "18  Was a patient hospitalized due to pump malfunc...  0.102463  18.552426   \n",
      "6   Are there cases where alarm failures are linke...  0.209299  22.723709   \n",
      "11  Are users describing unresponsive or non-funct...  0.108054  20.616260   \n",
      "12  Are users reporting device leaks or insulin de...  0.212555  27.460467   \n",
      "\n",
      "    ROUGE-L_F1  \n",
      "0     0.083333  \n",
      "18    0.087500  \n",
      "6     0.092105  \n",
      "11    0.092715  \n",
      "12    0.103896  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sacrebleu.metrics import CHRF\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "PRED_PATH = Path(\"llm_query_answer_topten.json\")\n",
    "REF_PATH  = Path(\"long_answer_ref.json\")\n",
    "OUT_CSV   = Path(\"score_analyse_long_ans.csv\")\n",
    "TOP_K_LOW = 5  # how many lowest-scoring questions to show\n",
    "\n",
    "# ---------------- LOAD ----------------\n",
    "pred_raw = json.loads(PRED_PATH.read_text(encoding=\"utf-8\"))\n",
    "ref_raw  = json.loads(REF_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Flatten nested structure\n",
    "def flatten(d):\n",
    "    flat = {}\n",
    "    for section, qa in d.items():\n",
    "        if isinstance(qa, dict):\n",
    "            for q, a in qa.items():\n",
    "                flat[q] = a.strip()\n",
    "    return flat\n",
    "\n",
    "preds = flatten(pred_raw)\n",
    "refs  = flatten(ref_raw)\n",
    "\n",
    "# Align keys exactly\n",
    "common_qs = sorted(set(preds.keys()) & set(refs.keys()))\n",
    "if len(common_qs) != len(refs):\n",
    "    print(f\"⚠ Missing keys — only {len(common_qs)}/{len(refs)} matched between preds and refs.\")\n",
    "\n",
    "y_pred = [preds[q] for q in common_qs]\n",
    "y_ref  = [refs[q]  for q in common_qs]\n",
    "\n",
    "# ---------------- METRICS ----------------\n",
    "# chrF++\n",
    "chrf = CHRF(word_order=2)  # chrF++\n",
    "chrf_scores = [chrf.sentence_score(hyp, [ref]).score for hyp, ref in zip(y_pred, y_ref)]\n",
    "\n",
    "# ROUGE-L\n",
    "rouger = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "rougeL_f = [rouger.score(ref, hyp)[\"rougeL\"].fmeasure for hyp, ref in zip(y_pred, y_ref)]\n",
    "\n",
    "# BERTScore (roberta-large baseline)\n",
    "P, R, F1 = bert_score(y_pred, y_ref, lang=\"en\", verbose=True, rescale_with_baseline=True, model_type=\"roberta-large\")\n",
    "P, R, F1 = P.tolist(), R.tolist(), F1.tolist()\n",
    "\n",
    "# ---------------- SAVE & ANALYZE ----------------\n",
    "df = pd.DataFrame({\n",
    "    \"question\": common_qs,\n",
    "    \"bert_P\": P,\n",
    "    \"bert_R\": R,\n",
    "    \"bert_F1\": F1,\n",
    "    \"chrF++\": chrf_scores,\n",
    "    \"ROUGE-L_F1\": rougeL_f,\n",
    "    \"prediction\": y_pred,\n",
    "    \"reference\": y_ref\n",
    "}).sort_values(\"bert_F1\", ascending=False)\n",
    "\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved full metrics to {OUT_CSV.resolve()}\")\n",
    "\n",
    "# Show worst questions by BERT F1\n",
    "print(\"\\n=== Lowest BERT F1 questions ===\")\n",
    "print(df.sort_values(\"bert_F1\").head(TOP_K_LOW)[[\"question\", \"bert_F1\", \"chrF++\", \"ROUGE-L_F1\"]])\n",
    "\n",
    "# Show worst questions by chrF++\n",
    "print(\"\\n=== Lowest chrF++ questions ===\")\n",
    "print(df.sort_values(\"chrF++\").head(TOP_K_LOW)[[\"question\", \"bert_F1\", \"chrF++\", \"ROUGE-L_F1\"]])\n",
    "\n",
    "# Show worst questions by ROUGE-L\n",
    "print(\"\\n=== Lowest ROUGE-L questions ===\")\n",
    "print(df.sort_values(\"ROUGE-L_F1\").head(TOP_K_LOW)[[\"question\", \"bert_F1\", \"chrF++\", \"ROUGE-L_F1\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aeb3f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: bert-score in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pandas in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: portalocker in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from sacrebleu) (2.10.1)\n",
      "Requirement already satisfied: regex in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from sacrebleu) (2024.11.6)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from sacrebleu) (2.2.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-6.0.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from bert-score) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from bert-score) (4.53.2)\n",
      "Requirement already satisfied: requests in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from bert-score) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from bert-score) (3.10.5)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from bert-score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from torch>=1.0.0->bert-score) (4.14.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from torch>=1.0.0->bert-score) (2025.7.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.33.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from matplotlib->bert-score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from matplotlib->bert-score) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from matplotlib->bert-score) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from matplotlib->bert-score) (3.2.3)\n",
      "Requirement already satisfied: click in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from nltk->rouge-score) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from nltk->rouge-score) (1.5.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from portalocker->sacrebleu) (311)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from requests->bert-score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from requests->bert-score) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shiva\\miniconda3\\envs\\llm_proj\\lib\\site-packages (from requests->bert-score) (2025.7.14)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading lxml-6.0.0-cp310-cp310-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.0/4.0 MB 21.7 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25027 sha256=6511b6d795a27ac188ed6232af3eced77bff6b1874a532eda70b121e7bf624fc\n",
      "  Stored in directory: c:\\users\\shiva\\appdata\\local\\pip\\cache\\wheels\\5f\\dd\\89\\461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: tabulate, lxml, absl-py, sacrebleu, rouge-score\n",
      "\n",
      "   -------- ------------------------------- 1/5 [lxml]\n",
      "   ------------------------ --------------- 3/5 [sacrebleu]\n",
      "   ---------------------------------------- 5/5 [rouge-score]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 lxml-6.0.0 rouge-score-0.1.2 sacrebleu-2.5.1 tabulate-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'rouge-score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge-score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu rouge-score bert-score pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
